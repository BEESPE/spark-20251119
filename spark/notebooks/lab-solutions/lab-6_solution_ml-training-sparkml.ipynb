{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab6MlApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b5e028-9f2a-44ab-846e-9e73e1bd6eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Charger les données cancer_reg (un header est présent, de plus, utiliser l'option inferSchema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63a4645-a40f-4db7-8949-2a4f7a2f8540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../../data/cancer_reg.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74cfb26-eda0-4eaa-8c8b-88225b013d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A l'aide de la fonction dir, afficher les méthodes pouvant être utilisées sur data. Afficher les 10 premières lignes du dataframe ainsi chargé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f702be7-7adf-4b6e-8e5f-d14e450b7d38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655246e5-724c-4623-b0be-86409d835038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2807f67e-fa99-47de-b4fa-a82949c7ea03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Supprimer du dataframe les colonnes binnedInc, Geography, PctSomeCol18_24, PctEmployed16_Over et PctPrivateCoverageAlone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cb3e00-2746-455b-962e-7f192bcd9a82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(\n",
    "    \"binnedInc\",\n",
    "    \"Geography\",\n",
    "    \"PctSomeCol18_24\",\n",
    "    \"PctEmployed16_Over\",\n",
    "    \"PctPrivateCoverageAlone\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14df318d-5a13-4d9b-9236-74b6b01a3371",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Repérer la colonne cible (target) et stocker dans des variables la liste des noms des colonnes des variables explicatives d'une part et le nom de la colonne cible d'autre part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f15a5a8-9956-4154-a458-079a541b3aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_col = \"TARGET_deathRate\"\n",
    "feature_cols = data.columns\n",
    "feature_cols.remove(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "facc19a3-0597-4741-88f7-82548e723e39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655d8f09-27c4-4790-946f-53e69dd1e8db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Séparer les données en un jeu d'entrainement et un jeu de test à l'aide de la méthode randomSplit. Choisir une répartition 80%-20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f0cb8b-5ac9-474e-9867-7a89da373563",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db856334-2412-4b1b-8d79-8f5643921b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c22b4da-4c64-45fb-9438-e88e77f03106",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15a9d1e-55bd-4e1c-a509-17f32831376f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assembler les features dans un vecteur colonne unique à l'aide de VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e98414c-731b-4d47-b7ff-d2ef90e5f773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5e2933-94e6-458f-a967-d814186af496",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instancier un modèle de régression linéaire en spécifiant les colonnes des variables explicatives et la colonne labellisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36258fc5-31ca-4488-b9a8-3613c366663f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "583245c1-c256-460e-a48e-4be36a8cd37e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Créer une pipeline contenant l'assembleur et du modèle de régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97465f46-47be-44e6-ae0c-eec2f6a0782d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318fc35b-9a59-4c09-84e4-232cdf20840e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Entraîner la pipeline sur le jeu d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1630a5f4-f428-4754-98b7-0c90421753be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "res = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3d1c7c-1800-46d5-89e4-14acd988fa62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Réaliser les prédictions sur le jeu de test. Afficher les 10 premières lignes avec leurs prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1901c336-53c7-4e28-a2ea-36fbf18de306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = res.transform(test_data)\n",
    "predictions.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f587374-7963-40c4-aa1e-5d438f81f022",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Créer un évaluateur correspondant au problème donné (en identifiant bien la typologie du problème). L'utiliser pour évaluer le rmse du modèle obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e89f647-b1a8-4a68-b8f6-2d791edca706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 20.067126521853375\n",
      "R2: 0.524183923681123\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ddcd92-bcf0-46b1-95ff-b34cbf97577e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reprendre les questions précédentes en rajoutant une étape d'ACP à 10 composantes entre le preprocessing et le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1e83d7-c241-419c-bf64-e63c4e71adf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f90d2b1-4681-4a80-870b-c04c280fd096",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 21.57858699571848\n",
      "R2: 0.44980740692937615\n"
     ]
    }
   ],
   "source": [
    "# assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "lr_2 = LinearRegression(featuresCol=\"pca_features\", labelCol=target_col)\n",
    "pipeline_2 = Pipeline(stages=[assembler, pca, lr_2])\n",
    "res_2 = pipeline_2.fit(train_data)\n",
    "predictions_2 = res_2.transform(test_data)\n",
    "#evaluator = RegressionEvaluator(\n",
    "#    labelCol=target_col,\n",
    "#    predictionCol=\"prediction\",\n",
    "#)\n",
    "rmse_2 = evaluator.evaluate(predictions_2, {evaluator.metricName: \"rmse\"})\n",
    "r2_2 = evaluator.evaluate(predictions_2, {evaluator.metricName: \"r2\"})\n",
    "print(f\"RMSE: {rmse_2}\")\n",
    "print(f\"R2: {r2_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b83eba-4e44-4c78-858d-b0d7e137f7fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conclure quant à l'utilité de l'ACP dans ce problème à dimension raisonnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêter la session Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2740810994971837,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SPARK_AVANCE_corr_ml-training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
