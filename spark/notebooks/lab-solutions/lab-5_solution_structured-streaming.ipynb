{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5776cc-1da2-46c2-82fc-a3db8e5a04c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Traitement de flux structuré avec l'API DataFrame Python\n",
    "Apache Spark inclut une API de traitement de flux de haut niveau : Structured Streaming.\n",
    "<br>Dans ce TP, nous jetons un coup d'œil rapide l'utilisation de l'API DataFrame pour construire des applications Structured Streaming.\n",
    "<br>Dans un premier temps, nous réviserons les éléments vus précédemment pour explorer rapidement les données.\n",
    "<br>Dans un second temps, nous calculerons des métriques en temps réel (dans notre exemple, ce seront des \"running counts\" et \"windowed counts\" sur un flux d'actions horodatées).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39eb54e7-59e7-4217-9619-79e4bf885027",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nous allons utiliser les 50 fichiers du dossier events. Chaque ligne de fichier contient un enregistrement json avec deux champs : `time` and `action`. Nous allons analyser ces fichiers de manière interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab5StreamingApp\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"../../spark-logs\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804bc3ee-1437-461b-8395-e9d18b32a8f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traitement par lots/interactif\n",
    "La première étape pour tenter de traiter les données est de les interroger de manière interactive.\n",
    "Nous allons définir un DataFrame statique sur les fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c4e93f-bd45-4a05-af7f-09eaf2ce8dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_path = \"../../data/events\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e832d292-85a9-42aa-aa14-d840bd833edb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Définir le schéma (au lieu de demander à Spark de l'inférer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8d523a-6c5f-46f7-9bec-e07b67cd1f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, TimestampType, StringType, StructField\n",
    "\n",
    "json_schema = StructType(\n",
    "    [\n",
    "        StructField(\"time\", TimestampType(), True),\n",
    "        StructField(\"action\", StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_df = spark.read.json(input_path, schema=json_schema)\n",
    "event_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-28 04:19:28</td>\n",
       "      <td>Close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-28 04:19:28</td>\n",
       "      <td>Close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-28 04:19:29</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-28 04:19:31</td>\n",
       "      <td>Close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-28 04:19:31</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2016-07-26 04:16:53</td>\n",
       "      <td>Close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>2016-07-26 04:17:01</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>2016-07-26 04:17:06</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>2016-07-26 04:17:10</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>2016-07-26 04:17:11</td>\n",
       "      <td>Close</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time action\n",
       "0     2016-07-28 04:19:28  Close\n",
       "1     2016-07-28 04:19:28  Close\n",
       "2     2016-07-28 04:19:29   Open\n",
       "3     2016-07-28 04:19:31  Close\n",
       "4     2016-07-28 04:19:31   Open\n",
       "...                   ...    ...\n",
       "99995 2016-07-26 04:16:53  Close\n",
       "99996 2016-07-26 04:17:01   Open\n",
       "99997 2016-07-26 04:17:06   Open\n",
       "99998 2016-07-26 04:17:10   Open\n",
       "99999 2016-07-26 04:17:11  Close\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5b179e-2575-4d6e-a05f-30949e5c9806",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Calculer le nombre d'actions \"open\" et \"close\" dans des fenêtres d'une heure.\n",
    "<br>Indication : utiliser un GROUP BY sur la colonne `action` et des fenêtres (windows) sur la colonne `time`.\n",
    "<br>Réaliser cette tâche en DSL.\n",
    "<br>Mettre en cache (en mémoire) le DataFrame obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f98cbc-749f-4206-9596-44d5340fd60f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "static_counts_df = event_df.groupBy(\n",
    "    col(\"action\"),\n",
    "    window(col(\"time\"), \"1 hour\")\n",
    ").count().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>window</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Close</td>\n",
       "      <td>(2016-07-26 13:00:00, 2016-07-26 14:00:00)</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open</td>\n",
       "      <td>(2016-07-27 04:00:00, 2016-07-27 05:00:00)</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Open</td>\n",
       "      <td>(2016-07-26 11:00:00, 2016-07-26 12:00:00)</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open</td>\n",
       "      <td>(2016-07-26 10:00:00, 2016-07-26 11:00:00)</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Close</td>\n",
       "      <td>(2016-07-27 03:00:00, 2016-07-27 04:00:00)</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Open</td>\n",
       "      <td>(2016-07-26 04:00:00, 2016-07-26 05:00:00)</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Close</td>\n",
       "      <td>(2016-07-26 03:00:00, 2016-07-26 04:00:00)</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Open</td>\n",
       "      <td>(2016-07-26 03:00:00, 2016-07-26 04:00:00)</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Open</td>\n",
       "      <td>(2016-07-26 02:00:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Close</td>\n",
       "      <td>(2016-07-26 02:00:00, 2016-07-26 03:00:00)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    action                                      window  count\n",
       "0    Close  (2016-07-26 13:00:00, 2016-07-26 14:00:00)   1028\n",
       "1     Open  (2016-07-27 04:00:00, 2016-07-27 05:00:00)    995\n",
       "2     Open  (2016-07-26 11:00:00, 2016-07-26 12:00:00)    991\n",
       "3     Open  (2016-07-26 10:00:00, 2016-07-26 11:00:00)   1007\n",
       "4    Close  (2016-07-27 03:00:00, 2016-07-27 04:00:00)   1025\n",
       "..     ...                                         ...    ...\n",
       "99    Open  (2016-07-26 04:00:00, 2016-07-26 05:00:00)    999\n",
       "100  Close  (2016-07-26 03:00:00, 2016-07-26 04:00:00)    344\n",
       "101   Open  (2016-07-26 03:00:00, 2016-07-26 04:00:00)   1001\n",
       "102   Open  (2016-07-26 02:00:00, 2016-07-26 03:00:00)    179\n",
       "103  Close  (2016-07-26 02:00:00, 2016-07-26 03:00:00)     11\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_counts_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3627362-e104-40da-b0f9-b764cb0388d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br>Déterminer le nombre total d'actions \"open\" et d'actions \"close\" (sur l'ensemble de la période).\n",
    "<br>Réaliser cette tâche en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26905ffd-c160-4c29-9f4c-41d79073d421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|action|total_count|\n",
      "+------+-----------+\n",
      "|  Open|      50000|\n",
      "| Close|      50000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_counts_df.createOrReplaceTempView(\"static_count_view\")\n",
    "\n",
    "static_total_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT action, sum(count) as total_count\n",
    "    FROM static_count_view\n",
    "    GROUP BY action\n",
    "    \"\"\"\n",
    ")\n",
    "static_total_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b663d7-aa35-4086-87a9-7fa963df2541",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Exécuter la commande suivante pour obtenir un visuel des actions dans le temps avec un affichage plus lisible de la plage d'heure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ba8161-4316-4084-96e8-a0857c2ec6fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "df_result = static_counts_df.select(\n",
    "    \"action\",\n",
    "    date_format(col(\"window\").start, \"MMM-dd HH:mm\").alias(\"time\"),\n",
    "    \"count\"\n",
    ").orderBy(\"time\", \"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 02:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 02:00</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 03:00</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 03:00</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 04:00</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-28 03:00</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-28 04:00</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-28 04:00</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-28 05:00</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-28 06:00</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    action          time  count\n",
       "0    Close  Jul-26 02:00     11\n",
       "1     Open  Jul-26 02:00    179\n",
       "2    Close  Jul-26 03:00    344\n",
       "3     Open  Jul-26 03:00   1001\n",
       "4    Close  Jul-26 04:00    815\n",
       "..     ...           ...    ...\n",
       "99    Open  Jul-28 03:00    996\n",
       "100  Close  Jul-28 04:00    960\n",
       "101   Open  Jul-28 04:00    825\n",
       "102  Close  Jul-28 05:00    671\n",
       "103  Close  Jul-28 06:00    191\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617e1996-8950-419f-a829-b1f815af2d83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vous devriez remarquer que les actions de fermeture suivent les actions d'ouverture correspondantes (il y a plus d'\"open\" au début et plus de \"close\" à la fin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda2a865-86b6-4963-af5f-385bd2334e25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Traitement de flux (stream processing)\n",
    "Maintenant que nous avons analysé les données de manière interactive, nous allons créer une requête de traitement de flux qui se met à jour en continu, à mesure que les données arrivent.\n",
    "<br>Étant donné que nous avons simplement un ensemble statique de fichiers, nous allons émuler un flux à partir d'eux en lisant un fichier à la fois, dans l'ordre chronologique de leur création.\n",
    "<br> A noter que le code que nous allons écrire est à peu près le même que précédemment. Merci l'API de Spark !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a471b3f-19a7-4e49-b0c0-94d7866dba20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Adapter le code créé précédemment pour calculer le nombre d'actions \"open\" et \"close\", en utilisant readStream à la place de read. Utiliser l'option maxFilesPerTrigger afin de spécifier qu'on ne prend qu'un ficher à la fois (les options doivent être placées avant le chargement effectif à partir de la méthode json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a940eea-2d4d-431c-ad2e-d9cedcb068d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_event_df = spark.readStream.option(\"maxFilesPerTrigger\", 1).json(input_path, schema=json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_counts_df = streaming_event_df.groupBy(\n",
    "    col(\"action\"),\n",
    "    window(col(\"time\"), \"1 hour\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nFileSource[../../data/events]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstreaming_counts_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nFileSource[../../data/events]"
     ]
    }
   ],
   "source": [
    "streaming_counts_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afd21d1-3389-4033-834f-735d0e81bb93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En fouillant (avec le nez fin) la documentation des DataFrames (https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html), vérifier si le DataFrame que nous venons de créer est bien un DataFrame de streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b810ba8-d21a-4402-b7f7-cd1091631384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_counts_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_counts_df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccbd0af-9c8c-4df2-9387-f1afc5edbcdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br>Vous pouvez démarrer le calcul en continu en définissant la destination (le \"sink\") et en le démarrant.\n",
    "<br>Dans notre cas, nous voulons interroger de manière interactive les comptages (mêmes requêtes que ci-dessus), nous allons donc définir l'ensemble complet des comptages d'une heure dans une table en mémoire (pour cela il faudra utiliser format avec l'option \"memory\").\n",
    "<br>Utiliser la fonction writeStream pour démarrer la requête.\n",
    "<br>Donner un nom à la table in-memory, afin de pouvoir y faire référence dans la suite. Pour cela, utiliser queryName.\n",
    "<br>Choisir un outputMode \"complete\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9e4efa-f07a-4474-a76c-70ef9ba85558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = streaming_counts_df.writeStream.format(\"memory\").queryName(\"streaming_counts\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36736a03-fed5-4173-9720-6d94659083ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`query` est une référence à la requête de streaming qui s'exécute en arrière-plan. Cette requête récupère continuellement les fichiers et met à jour les comptages fenêtrés. A noter que nous ne faisons que simuler une arrivée continue de fichiers, grâce à l'option maxFilesPerTrigger. Nous n'avons pas précisé d'intervalle de temps entre chaque trigger (il est possible de le paramétrer).\n",
    "\n",
    "Si vous travaillez avec l'IDE de Databricks, vous pouvez alors remarquer le statut de la requête dans la cellule ci-dessus (la barre de progression montrerait que la requête est active). Dans ce cas, développer `> counts` permet alors de retrouver le nombre de fichiers traités.\n",
    "\n",
    "Dans tous les cas, l'attribut isActive permet de vérifier si une query est active. Vérifier que notre query est bien active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f9c862-1a3b-461d-86af-bfceaa8df195",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Attendre un peu (physiquement ou en utilisant la fonction sleep de la bibliothèque time) puis lancer la requête sql permettant de visualiser les comptages dans le temps sur le DataFrame de streaming. Ne pas oublier que nous avons donné un nom particulier à la table in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fee0151-474e-44c5-90ae-24bd154dde16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result = static_counts_df.select(\n",
    "    \"action\",\n",
    "    date_format(col(\"window\").start, \"MMM-dd HH:mm\").alias(\"time\"),\n",
    "    \"count\"\n",
    ").orderBy(\"time\", \"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "    SELECT action, date_format(window.start, \"MMM-dd HH:mm\") as time, count\n",
    "    FROM streaming_counts\n",
    "    ORDER BY time, action\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 02:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 02:00</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 03:00</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 03:00</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 04:00</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 04:00</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action          time  count\n",
       "0  Close  Jul-26 02:00     11\n",
       "1   Open  Jul-26 02:00    179\n",
       "2  Close  Jul-26 03:00    344\n",
       "3   Open  Jul-26 03:00   1001\n",
       "4  Close  Jul-26 04:00    176\n",
       "5   Open  Jul-26 04:00    289"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(sql_query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4642a107-c2ac-460f-9bea-1d0f6f4b240d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Continuer à exécuter la requête de manière itérative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a730c8-66e0-41e3-a0e9-63dff4a52244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 02:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 02:00</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 03:00</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 03:00</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 04:00</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 04:00</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 05:00</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 05:00</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 13:00</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 13:00</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 14:00</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 14:00</td>\n",
       "      <td>991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Close</td>\n",
       "      <td>Jul-26 15:00</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Open</td>\n",
       "      <td>Jul-26 15:00</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action          time  count\n",
       "0   Close  Jul-26 02:00     11\n",
       "1    Open  Jul-26 02:00    179\n",
       "2   Close  Jul-26 03:00    344\n",
       "3    Open  Jul-26 03:00   1001\n",
       "4   Close  Jul-26 04:00    815\n",
       "5    Open  Jul-26 04:00    999\n",
       "6   Close  Jul-26 05:00    323\n",
       "7    Open  Jul-26 05:00    328\n",
       "8   Close  Jul-26 13:00    699\n",
       "9    Open  Jul-26 13:00    656\n",
       "10  Close  Jul-26 14:00    994\n",
       "11   Open  Jul-26 14:00    991\n",
       "12  Close  Jul-26 15:00    333\n",
       "13   Open  Jul-26 15:00    327"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(sql_query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5caecec-2eb9-4d1f-8694-89760b28abbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Visualiser le nombre total d'actions \"open\" et \"close\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb9131c-58b5-4d84-90f7-0dd5fc06f079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Open</td>\n",
       "      <td>50503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Close</td>\n",
       "      <td>51497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  action  total_count\n",
       "0   Open        50503\n",
       "1  Close        51497"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_total_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT action, sum(count) as total_count\n",
    "    FROM streaming_counts\n",
    "    GROUP BY action\n",
    "    \"\"\"\n",
    ")\n",
    "streaming_total_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cde8b1-799f-4244-bd32-148ec58260c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Si vous exécutez la requête ci-dessus de manière répétée, vous constaterez que le nombre d'actions \"open\" est bien en permanence plus élevé que le nombre d'actions \"close\", comme anticipable dans un flux de données où une fermeture apparaît toujours après l'ouverture qui lui correspondant. Cela montre que Structured Streaming garantit l'\"intégrité du préfixe\" (prefix integrity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9faaaf-19fb-464f-b2e2-9f7496392b40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Arrêter la requête en exécutant query.stop(). Vérifier le statut de la cellule correspondante à la requête.\n",
    "<br>Ici bien entendu, nous n'avons que quelques fichiers, et il est possible qu'au moment où vous arrivez à cette cellule, ils soient tous consommés. Dans ce cas, il n'y a plus de mise à jour des comptages. Mais il faut quand même arrêter notre requête proprement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020972e6-0686-4fea-8e6e-ef33190a2244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêter la requête n'arrête pas la session Spark. Arrêter la session Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2529895398287166,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "QNC_6_structured-streaming",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
