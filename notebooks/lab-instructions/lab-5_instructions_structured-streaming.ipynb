{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5776cc-1da2-46c2-82fc-a3db8e5a04c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Traitement de flux structuré avec l'API DataFrame Python\n",
    "Apache Spark inclut une API de traitement de flux de haut niveau : Structured Streaming.\n",
    "<br>Dans ce TP, nous jetons un coup d'œil rapide l'utilisation de l'API DataFrame pour construire des applications Structured Streaming.\n",
    "<br>Dans un premier temps, nous réviserons les éléments vus précédemment pour explorer rapidement les données.\n",
    "<br>Dans un second temps, nous calculerons des métriques en temps réel (dans notre exemple, ce seront des \"running counts\" et \"windowed counts\" sur un flux d'actions horodatées).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39eb54e7-59e7-4217-9619-79e4bf885027",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nous allons utiliser les 50 fichiers du dossier events. Chaque ligne de fichier contient un enregistrement json avec deux champs : `time` and `action`. Nous allons analyser ces fichiers de manière interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab5StreamingApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804bc3ee-1437-461b-8395-e9d18b32a8f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traitement par lots/interactif\n",
    "La première étape pour tenter de traiter les données est de les interroger de manière interactive.\n",
    "Nous allons définir un DataFrame statique sur les fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c4e93f-bd45-4a05-af7f-09eaf2ce8dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_path = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e832d292-85a9-42aa-aa14-d840bd833edb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pour changer (et en pratique car les formats de date ne seront pas inférés), définir le schéma au lieu de demander à Spark de l'inférer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8d523a-6c5f-46f7-9bec-e07b67cd1f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import # à compléter\n",
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5b179e-2575-4d6e-a05f-30949e5c9806",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Calculer le nombre d'actions \"open\" et \"close\" dans des fenêtres d'une heure.\n",
    "<br>Indication : utiliser un GROUP BY sur la colonne `action` et des fenêtres (windows) sur la colonne `time`.\n",
    "<br>Réaliser cette tâche en DSL.\n",
    "<br>Mettre en cache (en mémoire) le DataFrame obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77f98cbc-749f-4206-9596-44d5340fd60f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "static_counts_df = ...\n",
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3627362-e104-40da-b0f9-b764cb0388d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br>Déterminer le nombre total d'actions \"open\" et d'actions \"close\" (sur l'ensemble de la période).\n",
    "<br>Réaliser cette tâche en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26905ffd-c160-4c29-9f4c-41d79073d421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b663d7-aa35-4086-87a9-7fa963df2541",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Exécuter la commande suivante pour obtenir un visuel des actions dans le temps avec un affichage plus lisible de la plage d'heure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ba8161-4316-4084-96e8-a0857c2ec6fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "df_result = static_counts_df.select(\n",
    "    \"action\",\n",
    "    date_format(col(\"window\").start, \"MMM-dd HH:mm\").alias(\"time\"),\n",
    "    \"count\"\n",
    ").orderBy(\"time\", \"action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617e1996-8950-419f-a829-b1f815af2d83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vous devriez remarquer que les actions de fermeture suivent les actions d'ouverture correspondantes (il y a plus d'\"open\" au début et plus de \"close\" à la fin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda2a865-86b6-4963-af5f-385bd2334e25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Traitement de flux (stream processing)\n",
    "Maintenant que nous avons analysé les données de manière interactive, nous allons créer une requête de traitement de flux qui se met à jour en continu, à mesure que les données arrivent.\n",
    "<br>Étant donné que nous avons simplement un ensemble statique de fichiers, nous allons émuler un flux à partir d'eux en lisant un fichier à la fois, dans l'ordre chronologique de leur création.\n",
    "<br> A noter que le code que nous allons écrire est à peu près le même que précédemment. Merci l'API de Spark !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a471b3f-19a7-4e49-b0c0-94d7866dba20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Adapter le code créé précédemment pour calculer le nombre d'actions \"open\" et \"close\", en utilisant readStream à la place de read. Utiliser l'option maxFilesPerTrigger afin de spécifier qu'on ne prend qu'un ficher à la fois (les options doivent être placées avant le chargement effectif à partir de la méthode json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a940eea-2d4d-431c-ad2e-d9cedcb068d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afd21d1-3389-4033-834f-735d0e81bb93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En fouillant (avec le nez fin) la documentation des DataFrames (https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.html?highlight=dataframe#pyspark.sql.DataFrame), vérifier si le DataFrame que nous venons de créer est bien un DataFrame de streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b810ba8-d21a-4402-b7f7-cd1091631384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccbd0af-9c8c-4df2-9387-f1afc5edbcdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br>Vous pouvez démarrer le calcul en continu en définissant la destination (le \"sink\") et en le démarrant.\n",
    "<br>Dans notre cas, nous voulons interroger de manière interactive les comptages (mêmes requêtes que ci-dessus), nous allons donc définir l'ensemble complet des comptages d'une heure dans une table en mémoire (pour cela il faudra utiliser format avec l'option \"memory\").\n",
    "<br>Utiliser la fonction writeStream pour démarrer la requête.\n",
    "<br>Donner un nom à la table in-memory, afin de pouvoir y faire référence dans la suite. Pour cela, utiliser queryName.\n",
    "<br>Choisir un outputMode \"complete\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9e4efa-f07a-4474-a76c-70ef9ba85558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36736a03-fed5-4173-9720-6d94659083ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`query` est une référence à la requête de streaming qui s'exécute en arrière-plan. Cette requête récupère continuellement les fichiers et met à jour les comptages fenêtrés. A noter que nous ne faisons que simuler une arrivée continue de fichiers, grâce à l'option maxFilesPerTrigger. Nous n'avons pas précisé d'intervalle de temps entre chaque trigger (il est possible de le paramétrer).\n",
    "\n",
    "Si vous travaillez avec l'IDE de Databricks, vous pouvez alors remarquer le statut de la requête dans la cellule ci-dessus (la barre de progression montrerait que la requête est active). Dans ce cas, développer `> counts` permet alors de retrouver le nombre de fichiers traités.\n",
    "\n",
    "Dans tous les cas, l'attribut isActive permet de vérifier si une query est active. Vérifier que notre query est bien active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f9c862-1a3b-461d-86af-bfceaa8df195",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Attendre un peu (physiquement ou en utilisant la fonction sleep de la bibliothèque time) puis lancer la requête sql permettant de visualiser les comptages dans le temps sur le DataFrame de streaming. Ne pas oublier que nous avons donné un nom particulier à la table in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fee0151-474e-44c5-90ae-24bd154dde16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4642a107-c2ac-460f-9bea-1d0f6f4b240d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Continuer à exécuter la requête de manière itérative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a730c8-66e0-41e3-a0e9-63dff4a52244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code- (éventuel) ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5caecec-2eb9-4d1f-8694-89760b28abbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Visualiser le nombre total d'actions \"open\" et \"close\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb9131c-58b5-4d84-90f7-0dd5fc06f079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cde8b1-799f-4244-bd32-148ec58260c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Si vous exécutez la requête ci-dessus de manière répétée, vous constaterez que le nombre d'actions \"open\" est bien en permanence plus élevé que le nombre d'actions \"close\", comme anticipable dans un flux de données où une fermeture apparaît toujours après l'ouverture qui lui correspondant. Cela montre que Structured Streaming garantit l'\"intégrité du préfixe\" (prefix integrity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9faaaf-19fb-464f-b2e2-9f7496392b40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Arrêter la requête en exécutant query.stop(). Vérifier le statut de la cellule correspondante à la requête.\n",
    "<br>Ici bien entendu, nous n'avons que quelques fichiers, et il est possible qu'au moment où vous arrivez à cette cellule, ils soient tous consommés. Dans ce cas, il n'y a plus de mise à jour des comptages. Mais il faut quand même arrêter notre requête proprement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020972e6-0686-4fea-8e6e-ef33190a2244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêter la requête n'arrête pas la session Spark. Arrêter la session Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2529895398287166,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "QNC_6_structured-streaming",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
